{"metadata":{"kernelspec":{"display_name":"JavaScript","language":"javascript","name":"javascript"},"language_info":{"codemirror_mode":{"name":"javascript","version":6},"file_extension":".js","mimetype":"text/javascript","name":"javascript","nbconvert_exporter":"javascript"}},"nbformat":4,"nbformat_minor":2,"cells":[{"cell_type":"markdown","metadata":{},"source":"## MicroGrad by Andrej Karpathy\n[Video](https://www.youtube.com/watch?v=VMj-3S1tku0), [Source](https://github.com/karpathy/micrograd)\n\nTranslated from Python to JavaScript by Joshua Moore\n\nTuesday, February 4th, 2025"},{"cell_type":"markdown","metadata":{},"source":"The value class represents a single number to be used in a calculation, along with the capability of detecting this value's influence on the output of a calculation, its gradient.\n\nSeveral mathematical operations are defined on the Value, the key for adding more being to also define a `_backward` function on the return value that can be used to calculate the gradient. "},{"cell_type":"code","execution_count":1,"metadata":{},"source":"Value = class Value {\n  constructor(data, children=[], op='', label=''){\n    this.data = data;\n    this.prev = new Set(children);\n    this.op = op;\n    this.label = label;\n    this.grad = 0;\n    this._backward = () => {};\n  }\n\n  toString(){\n    return `Value ${this.label + ' '}(data=${this.data.toFixed(3)} grad=${this.grad.toFixed(3)})`;\n  }\n\n  add(other){\n    other = other instanceof Value ? other : new Value(other);\n    const out = new Value(this.data + other.data, [this, other], '+');\n    out._backward = () => {\n      this.grad += out.grad;\n      other.grad += out.grad;\n    };\n    \n    return out;\n  }\n\n  mul(other){\n    other = other instanceof Value ? other : new Value(other)\n    const out = new Value(this.data * other.data, [this, other], '*');\n    out._backward = () => {\n      this.grad += other.data * out.grad;\n      other.grad += this.data * out.grad;\n    }\n    \n    return out;\n  }\n\n  pow(other){\n    console.assert(`Argument other must be a Number.`, other instanceof Number);\n    const out = new Value(this.data**other, [this], `**${other}`);\n    out._backward = () => {\n      this.grad += (other * this.data**(other-1)) * out.grad;\n    }\n\n    return out;\n  }\n\n  relu(){\n    const out = new Value(this.data < 0 ? 0 : this.data, [this], 'ReLu');\n    out._backward = () => {\n      this.grad += (out.data > 0 ? 1 : 0) * out.grad;\n    }\n\n    return out;\n  }\n\n  backward(){\n    const topo = [];\n    const visited = new Set();\n    const buildTopo = (v) => {\n      if(!visited.has(v)){\n        visited.add(v);\n        for(let child of v.prev){\n          buildTopo(child);\n        }\n        topo.push(v);\n      }\n    };\n    buildTopo(this);\n\n    this.grad = 1.0;\n    for(let i = topo.length -1; i>=0; i--){\n      topo[i]._backward();\n    }\n    \n    return topo;\n  }\n\n  neg(){\n    return this.mul(-1);\n  }\n\n  sub(other){\n    other = other instanceof Value ? other : new Value(other);\n    return this.add(other.neg());\n  }\n\n  div(other){\n    return this.mul(other.pow(-1));\n  }\n\n  tanh(){\n    const x = this.data;\n    const t = (Math.exp(2*x) - 1)/(Math.exp(2*x) + 1);\n    const out = new Value(t, [this], 'tanh');\n    out._backward = () => {\n      this.grad += (1- t**2) * out.grad;\n    }\n\n    return out;\n  }\n\n  exp(){\n    const x = this.data;\n    const out = new Value(Math.exp(x), [this], 'exp');\n    out._backward = () => {\n      this.grad += out.data * out.grad;\n    }\n\n    return out;\n  }\n\n  trace(){\n    const nodes = new Set(), edges = new Set();\n    const build = (v) => {\n      nodes.add(v);\n      for(let child of v.prev){\n        edges.add([child, v]);\n        build(child);\n      }\n    }\n    build(root);\n    return nodes, edges;\n  }\n}","outputs":[{"name":"stdout","output_type":"stream","text":[]},{"data":{"text/plain":""},"execution_count":1,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"markdown","metadata":{},"source":"A Neuron is a collection of values and weights."},{"cell_type":"code","execution_count":1,"metadata":{},"source":"Neuron = class Neuron {\n  constructor(nin, nonlin=true){\n    this.w = Array(nin).fill(0).map(() => new Value((Math.random() * 2) - 1));\n    this.b = new Value(0);\n    this.nonlin = nonlin;\n  }\n\n  call(x){\n    const act = this.w\n      .filter((w, i) => i >= this.b.data)\n      .map((wi, i) => [wi, x[i]])\n      .reduce((acc, pair) => {\n        return acc.add(pair[0].mul(pair[1]))\n      }, new Value(0));\n    \n    const out = act.tanh();\n    return out;\n  }\n\n  parameters(){\n    return this.w.concat([this.b]);\n  }\n\n  toString(){\n    return `${this.nonlin ? 'ReLu' : 'Linear'} Neuron(${this.w.length})`;\n  }\n}","outputs":[{"name":"stdout","output_type":"stream","text":[]},{"data":{"text/plain":""},"execution_count":1,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"markdown","metadata":{},"source":"A layer is a collection of Neurons"},{"cell_type":"code","execution_count":1,"metadata":{},"source":"Layer = class Layer {\n  constructor(nin, nout, nonlin){\n    this.neurons = Array(nout).fill(0).map(() => new Neuron(nin, nonlin));\n  }\n\n  call(x){\n    const outs = this.neurons.map(n => n.call(x));\n    return outs.length == 1 ? outs[0] : outs;\n  }\n\n  parameters(){\n    const params = [];\n    for(let neuron of this.neurons){\n      for(let p of neuron.parameters()){\n        params.push(p);\n      }\n    }\n    \n    return params;\n  }\n\n  toString(){\n    return `Layer of [{${this.neurons.map(n => n.toString()).join(', ')}}]`\n  }\n}","outputs":[{"name":"stdout","output_type":"stream","text":[]},{"data":{"text/plain":""},"execution_count":1,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"markdown","metadata":{},"source":"A multilayer perceptron consists of a number of layers."},{"cell_type":"code","execution_count":1,"metadata":{},"source":"MLP = class MLP {\n  constructor(nin, nouts){\n    const sz = [nin].concat(...nouts);\n    this.layers = Array(nouts.length).fill(0).map((_, i) => new Layer(sz[i], sz[i+1], i < nouts.length - 1))\n  }\n\n  call(x){\n    for(let layer of this.layers){\n      x = layer.call(x);\n    }\n    \n    return x;\n  }\n\n  parameters(){\n    let params = [];\n    for(let layer of this.layers){\n      for(let p of layer.parameters()){\n        params.push(p);\n      }\n    }\n\n    return params;\n  }\n\n  toString(){\n    return `MLP of [{${this.layers.map(l => l.toString()).join(', ')}}]`\n  }\n}","outputs":[{"name":"stdout","output_type":"stream","text":[]},{"data":{"text/plain":""},"execution_count":1,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"code","execution_count":2,"metadata":{},"source":"let x = [2.0, 3.0, -1.0]\nlet mlp = new MLP(3, [4, 4, 1]);\nlet rounds = 1000;\nmlp.call(x)\n\n// inputs\nlet xs =[\n  [2.0, 3.0, -1.0],\n  [3.0, -1.0, 0.5],\n  [0.5, 1.0, 1.0],\n  [1.0, 1.0, -1.0]\n];\n\n// correct (training) outputs\nlet ys = [1.0, -1.0, -1.0, 1.0];\n\nconsole.log(`target values:<br>`, JSON.stringify(ys));\n\n// prediction by the MLP\nlet predictions = xs.map((x) => mlp.call(x));\n\n// essentially beefed up difference between correct outputs and mlp outputs\nlet loss = ys\n  .map((ygt, i) => predictions[i].sub(ygt).pow(2))\n  .reduce((acc, l) => acc.add(l), new Value(0));\n\n// calculate the gradients of all expressions' nodes\nloss.backward()\n\n// log values to output so we can see\nconsole.log(`<br>prediction before optimization:<br>`, JSON.stringify(predictions.map(y => parseFloat(y.data.toFixed(3))), null, 2));\nconsole.log(`<br>starting loss:<br> ${loss.data.toFixed(3)}`);\n\n// optimization\n// adjust the mlp's parameters in the direction (up or down) multiplied by\n// some small constant to control the size of the variation steps.\nfor(var i=0; i<1000; i++){\n  // mlp predict for each input\n  predictions = xs.map((x) => mlp.call(x));\n  loss = ys\n    .map((ygt, i) => predictions[i].sub(ygt).pow(2))\n    .reduce((acc, l) => acc.add(l), new Value(0));\n\n  // reset the gradients (not sure why, but it was explained)\n  let params = mlp.parameters();\n  for(let p of params){\n    p.grad = 0.0;\n  }\n\n  // calculate the mlp's .grads\n  loss.backward();\n\n  // set value of p to -0.01 * p.grad.\n  // - - because we want the value to change in the direction of less wrong\n  // 0.01 some small step size, chosen to neither over-step, nor take too long.\n  // p.grad is an arrow \n  params = mlp.parameters();\n  for(let p of params){\n    p.data += -0.01 * p.grad;\n  }\n\n  // console.info(`${i} ${loss.data}`)\n}\n\nconsole.log(`<br>rounds of optimization:<br>${rounds}`)\nconsole.log(`<br>predictions after optimization:<br>`, JSON.stringify(predictions.map(y => parseFloat(y.data.toFixed(3))), null, 2));\nconsole.log(`<br>final loss:<br> ${loss.data.toFixed(3)}`);","outputs":[{"name":"stdout","output_type":"stream","text":["target values:<br> [1,-1,-1,1]","<br>prediction before optimization:<br> [\n  -0.005,\n  -0.098,\n  -0.302,\n  0.146\n]","<br>starting loss:<br> 3.041","<br>rounds of optimization:<br>1000","<br>predictions after optimization:<br> [\n  0.971,\n  -0.977,\n  -0.971,\n  0.974\n]","<br>final loss:<br> 0.003"]},{"data":{"text/plain":""},"execution_count":2,"metadata":{},"output_type":"execute_result"}]}]}